{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas openpyxl --quiet"
      ],
      "metadata": {
        "id": "R7MyzzppNmb7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASET PREPARATION"
      ],
      "metadata": {
        "id": "e2f1x4rCZ3YL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "products_df = pd.read_excel('/content/synthetic_products.xlsx')\n",
        "promotions_df = pd.read_excel('/content/synthetic_promotions.xlsx')\n",
        "users_df = pd.read_excel('/content/synthetic_users.xlsx')\n",
        "transactions_df = pd.read_excel('/content/synthetic_transactions.xlsx')\n",
        "\n",
        "def parse_list(list_str):\n",
        "    try:\n",
        "        return eval(list_str.replace(\"'\", \"\"))\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "promotions_df['eligible_users'] = promotions_df['eligible_users'].apply(parse_list)\n",
        "promotions_df['products_covered'] = promotions_df['products_covered'].apply(parse_list)\n",
        "\n",
        "full_df = (\n",
        "    transactions_df\n",
        "    .merge(users_df, on='user_id')\n",
        "    .merge(products_df, on='product_id')\n",
        "    .merge(promotions_df, left_on='promotion_used', right_on='promotion_id', how='left')\n",
        ")\n",
        "\n",
        "STATE_ATTRIBUTES = [\n",
        "    'user_id',\n",
        "    'product_id',\n",
        "    'category',\n",
        "    'quantity',\n",
        "    'response',\n",
        "    'churn_score',\n",
        "    'loyalty_score',\n",
        "    'avg_order_value',\n",
        "    'price',\n",
        "    'margin',\n",
        "    'promotion_history',\n",
        "    'age',\n",
        "    'gender',\n",
        "    'income',\n",
        "    'segment',\n",
        "    'promotion_used'\n",
        "]\n",
        "\n",
        "final_dataset = full_df[STATE_ATTRIBUTES]\n",
        "\n",
        "final_dataset.to_csv('all_users_state_actions.csv', index=False)\n",
        "\n",
        "print(f\"Saved {len(final_dataset)} entries with {len(final_dataset.columns)} state attributes\")\n",
        "print(\"Columns order:\", list(final_dataset.columns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BR_-qGoCQ7f6",
        "outputId": "ac3caadf-c3f7-41b7-c23c-e51b2ab8a32c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 300 entries with 16 state attributes\n",
            "Columns order: ['user_id', 'product_id', 'category', 'quantity', 'response', 'churn_score', 'loyalty_score', 'avg_order_value', 'price', 'margin', 'promotion_history', 'age', 'gender', 'income', 'segment', 'promotion_used']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA PRE-PROCESSING"
      ],
      "metadata": {
        "id": "pKZHXOHcZ-aX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv('/content/all_users_state_actions.csv')\n",
        "\n",
        "print(\"Missing Values Before Processing:\")\n",
        "missing_values = df.isnull().sum()\n",
        "percent_missing = (missing_values/len(df)) * 100\n",
        "missing_table = pd.DataFrame({\n",
        "    'Missing Values': missing_values,\n",
        "    'Percentage (%)': percent_missing.round(2)\n",
        "})\n",
        "print(missing_table)\n",
        "\n",
        "def handle_missing(data):\n",
        "    cleaned_df = data.copy()\n",
        "\n",
        "    num_cols = ['quantity', 'churn_score', 'loyalty_score',\n",
        "               'avg_order_value', 'price', 'margin', 'income']\n",
        "    for col in num_cols:\n",
        "        if col in cleaned_df.columns:\n",
        "\n",
        "            cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].median())\n",
        "\n",
        "    cat_cols = ['category', 'gender', 'segment', 'promotion_history']\n",
        "    for col in cat_cols:\n",
        "        if col in cleaned_df.columns:\n",
        "\n",
        "            cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].mode()[0])\n",
        "\n",
        "    if 'promotion_used' in cleaned_df.columns:\n",
        "        cleaned_df['promotion_used'] = cleaned_df['promotion_used'].fillna(0)\n",
        "\n",
        "    cleaned_df = cleaned_df.dropna()\n",
        "\n",
        "    return cleaned_df\n",
        "\n",
        "cleaned_df = handle_missing(df)\n",
        "\n",
        "print(\"\\nMissing Values After Processing:\")\n",
        "print(cleaned_df.isnull().sum())\n",
        "\n",
        "cleaned_df.to_csv('cleaned_state_actions.csv', index=False)\n",
        "print(\"\\nCleaned dataset saved as cleaned_state_actions.csv\")\n",
        "\n",
        "original_rows = len(df)\n",
        "cleaned_rows = len(cleaned_df)\n",
        "print(f\"\\nData Cleaning Report:\")\n",
        "print(f\"Original entries: {original_rows}\")\n",
        "print(f\"Cleaned entries: {cleaned_rows}\")\n",
        "print(f\"Rows removed: {original_rows - cleaned_rows} ({(1-(cleaned_rows/original_rows))*100:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0CCuzozaCUL",
        "outputId": "91b0d7ef-8edb-4323-8c39-536c8b6d56a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Values Before Processing:\n",
            "                   Missing Values  Percentage (%)\n",
            "user_id                         0            0.00\n",
            "product_id                      0            0.00\n",
            "category                        0            0.00\n",
            "quantity                        0            0.00\n",
            "response                        0            0.00\n",
            "churn_score                     0            0.00\n",
            "loyalty_score                   0            0.00\n",
            "avg_order_value                 0            0.00\n",
            "price                           0            0.00\n",
            "margin                          0            0.00\n",
            "promotion_history               0            0.00\n",
            "age                             0            0.00\n",
            "gender                          0            0.00\n",
            "income                          0            0.00\n",
            "segment                         0            0.00\n",
            "promotion_used                 79           26.33\n",
            "\n",
            "Missing Values After Processing:\n",
            "user_id              0\n",
            "product_id           0\n",
            "category             0\n",
            "quantity             0\n",
            "response             0\n",
            "churn_score          0\n",
            "loyalty_score        0\n",
            "avg_order_value      0\n",
            "price                0\n",
            "margin               0\n",
            "promotion_history    0\n",
            "age                  0\n",
            "gender               0\n",
            "income               0\n",
            "segment              0\n",
            "promotion_used       0\n",
            "dtype: int64\n",
            "\n",
            "Cleaned dataset saved as cleaned_state_actions.csv\n",
            "\n",
            "Data Cleaning Report:\n",
            "Original entries: 300\n",
            "Cleaned entries: 300\n",
            "Rows removed: 0 (0.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASET SPLITTING"
      ],
      "metadata": {
        "id": "05U1XLpz9_Fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/cleaned_state_actions.csv')  # Replace with your filename\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "train_df.to_csv('train_data.csv', index=False)\n",
        "test_df.to_csv('test_data.csv', index=False)\n",
        "\n",
        "!ls -l *.csv\n",
        "\n",
        "files.download('train_data.csv')\n",
        "files.download('test_data.csv')\n",
        "\n",
        "print(\"Training samples:\", len(train_df))\n",
        "print(\"Test samples:\", len(test_df))\n",
        "print(\"Files saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "W5THiHb79-M3",
        "outputId": "2342b459-4e67-4aa8-c85e-4977bfbf1cf3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 26175 May  8 15:18 all_users_state_actions.csv\n",
            "-rw-r--r-- 1 root root 26254 May  8 15:18 cleaned_state_actions.csv\n",
            "-rw-r--r-- 1 root root  5357 May  8 15:18 test_data.csv\n",
            "-rw-r--r-- 1 root root 21057 May  8 15:18 train_data.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5b245974-ff01-45a7-b7d0-5d60a0c1f0c7\", \"train_data.csv\", 21057)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9d4ed0a2-6cfd-4ad0-8927-a4d5a6455d80\", \"test_data.csv\", 5357)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 240\n",
            "Test samples: 60\n",
            "Files saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RL AGENT TRAINING (QDN ALGORITHM)"
      ],
      "metadata": {
        "id": "jgTz1OCIag4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import deque\n",
        "import random\n",
        "import math\n",
        "\n",
        "PROMOTION_IDS = ['PR100', 'PR101', 'PR102', 'PR103', 'PR104',\n",
        "                'PR105', 'PR106', 'PR107', 'PR108', 'PR109']\n",
        "ACTION_SPACE_SIZE = len(PROMOTION_IDS)\n",
        "\n",
        "class AdvancedRewardSystem:\n",
        "    def __init__(self, config):\n",
        "        self.weights = config['weights']\n",
        "        self.segment_multipliers = config['segment_multipliers']\n",
        "        self.promo_costs = {\n",
        "            'PR100': 0.15, 'PR101': 0.20, 'PR102': 0.18,\n",
        "            'PR103': 0.25, 'PR104': 0.22, 'PR105': 0.19,\n",
        "            'PR106': 0.17, 'PR107': 0.21, 'PR108': 0.23,\n",
        "            'PR109': 0.16\n",
        "        }\n",
        "        self.eligibility_map = {\n",
        "            'Low': ['PR100', 'PR101', 'PR102'],\n",
        "            'Medium': ['PR103', 'PR104', 'PR105'],\n",
        "            'High': ['PR106', 'PR107', 'PR108', 'PR109']\n",
        "        }\n",
        "\n",
        "    def get_valid_actions(self, promotion_history):\n",
        "        eligible = self.eligibility_map.get(promotion_history, [])\n",
        "        return [PROMOTION_IDS.index(p) for p in eligible if p in PROMOTION_IDS]\n",
        "\n",
        "    def calculate_reward(self, transaction, action):\n",
        "        promo_id = PROMOTION_IDS[action]\n",
        "        cost = self.promo_costs[promo_id]\n",
        "\n",
        "        response = transaction['response']\n",
        "        quantity = transaction['quantity']\n",
        "        price = transaction['price']\n",
        "        margin = transaction['margin'] - cost\n",
        "        loyalty = transaction['loyalty_score']\n",
        "        churn = transaction['churn_score']\n",
        "        segment = transaction.get('segment', 'Standard')\n",
        "\n",
        "        sales_reward = math.log1p(quantity * price) * response * 2.0\n",
        "        margin_reward = margin * quantity * 0.5\n",
        "        retention_reward = (1 / (1 + math.exp(-loyalty))) - math.exp(churn)\n",
        "        penalty = (1 - response) * price * 0.3\n",
        "\n",
        "        segment_mult = self.segment_multipliers.get(segment, 1.0)\n",
        "\n",
        "        return ((self.weights['sales'] * sales_reward +\n",
        "                self.weights['margin'] * margin_reward +\n",
        "                self.weights['retention'] * retention_reward -\n",
        "                penalty) * segment_mult)\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = ACTION_SPACE_SIZE\n",
        "        self.memory = deque(maxlen=20000)\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.985\n",
        "        self.epsilon_min = 0.15\n",
        "        self.batch_size = 256\n",
        "\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.update_target()\n",
        "        self.optimizer = optim.AdamW(self.model.parameters(), lr=0.001)\n",
        "        self.loss_fn = nn.SmoothL1Loss()\n",
        "\n",
        "    def _build_model(self):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(self.state_size, 512),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(256, self.action_size)\n",
        "        )\n",
        "\n",
        "    def act(self, state, valid_actions):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.choice(valid_actions)\n",
        "\n",
        "        state = torch.FloatTensor(state)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.model(state)\n",
        "\n",
        "        valid_q = q_values[valid_actions]\n",
        "        return valid_actions[torch.argmax(valid_q).item()]\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        states = torch.FloatTensor(np.array([x[0] for x in minibatch]))\n",
        "        actions = torch.LongTensor([x[1] for x in minibatch])\n",
        "        rewards = torch.FloatTensor([x[2] for x in minibatch])\n",
        "        next_states = torch.FloatTensor(np.array([x[3] for x in minibatch]))\n",
        "        dones = torch.FloatTensor([x[4] for x in minibatch])\n",
        "\n",
        "        current_q = self.model(states).gather(1, actions.unsqueeze(1))\n",
        "        next_q = self.target_model(next_states).max(1)[0].detach()\n",
        "        target_q = rewards + (1 - dones) * self.gamma * next_q\n",
        "\n",
        "        loss = self.loss_fn(current_q.squeeze(), target_q)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "def preprocess_data(df):\n",
        "    df = df.drop(['user_id', 'product_id', 'promotion_used'], axis=1, errors='ignore')\n",
        "    categorical_cols = ['category', 'gender', 'segment', 'promotion_history']\n",
        "    df = pd.get_dummies(df, columns=categorical_cols)\n",
        "    numeric_cols = ['quantity', 'price', 'margin', 'churn_score',\n",
        "                   'loyalty_score', 'avg_order_value', 'age', 'income']\n",
        "    for col in numeric_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = (df[col] - df[col].mean()) / df[col].std()\n",
        "\n",
        "    return df.fillna(0)\n",
        "\n",
        "def train_dqn_agent(df, config):\n",
        "    df = df[df['promotion_used'].isin(PROMOTION_IDS)].copy()\n",
        "    processed_df = preprocess_data(df)\n",
        "    promotion_map = {p: i for i, p in enumerate(PROMOTION_IDS)}\n",
        "    processed_df['action'] = df['promotion_used'].map(promotion_map)\n",
        "    state_columns = [col for col in processed_df.columns if col != 'action']\n",
        "    states = processed_df[state_columns].values.astype(np.float32)\n",
        "    states = np.nan_to_num(states)\n",
        "    reward_system = AdvancedRewardSystem(config)\n",
        "    agent = DQNAgent(states.shape[1])\n",
        "    episodes = 300\n",
        "    steps_per_episode = 300\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        total_reward = 0\n",
        "        indices = np.random.permutation(len(processed_df))\n",
        "\n",
        "        for step in range(steps_per_episode):\n",
        "            idx = indices[step % len(processed_df)]\n",
        "            row = df.iloc[idx]\n",
        "            state = states[idx]\n",
        "            valid_actions = reward_system.get_valid_actions(row['promotion_history'])\n",
        "            if not valid_actions:\n",
        "                continue\n",
        "            action = agent.act(state, valid_actions)\n",
        "            reward = reward_system.calculate_reward(row.to_dict(), action)\n",
        "            next_idx = (idx + 1) % len(processed_df)\n",
        "            next_state = states[next_idx]\n",
        "            agent.remember(state, action, reward, next_state, False)\n",
        "\n",
        "\n",
        "            if step % 2 == 0:\n",
        "                agent.replay()\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "        agent.update_target()\n",
        "        agent.decay_epsilon()\n",
        "\n",
        "        avg_reward = total_reward / steps_per_episode\n",
        "        print(f\"Episode {episode+1:03d}/{episodes} | Avg Reward: {avg_reward:.2f} | ε: {agent.epsilon:.3f}\")\n",
        "\n",
        "    return agent\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = pd.read_csv('train_data.csv')\n",
        "\n",
        "    reward_config = {\n",
        "        'weights': {\n",
        "            'sales': 0.4,\n",
        "            'margin': 0.35,\n",
        "            'retention': 0.25\n",
        "        },\n",
        "        'segment_multipliers': {\n",
        "            'Premium': 1.6,\n",
        "            'Standard': 1.1,\n",
        "            'Budget': 0.7\n",
        "        }\n",
        "    }\n",
        "\n",
        "    trained_agent = train_dqn_agent(df, reward_config)\n",
        "    torch.save(trained_agent.model.state_dict(), 'final_promotion_model.pth')\n",
        "    print(\"Training completed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_Oe9sOOam1T",
        "outputId": "8441090b-8278-462f-c54a-ff3f24519552"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 001/300 | Avg Reward: 37.95 | ε: 0.985\n",
            "Episode 002/300 | Avg Reward: 38.04 | ε: 0.970\n",
            "Episode 003/300 | Avg Reward: 37.06 | ε: 0.956\n",
            "Episode 004/300 | Avg Reward: 38.24 | ε: 0.941\n",
            "Episode 005/300 | Avg Reward: 37.09 | ε: 0.927\n",
            "Episode 006/300 | Avg Reward: 37.65 | ε: 0.913\n",
            "Episode 007/300 | Avg Reward: 37.00 | ε: 0.900\n",
            "Episode 008/300 | Avg Reward: 37.41 | ε: 0.886\n",
            "Episode 009/300 | Avg Reward: 38.08 | ε: 0.873\n",
            "Episode 010/300 | Avg Reward: 37.43 | ε: 0.860\n",
            "Episode 011/300 | Avg Reward: 37.94 | ε: 0.847\n",
            "Episode 012/300 | Avg Reward: 37.65 | ε: 0.834\n",
            "Episode 013/300 | Avg Reward: 37.77 | ε: 0.822\n",
            "Episode 014/300 | Avg Reward: 37.88 | ε: 0.809\n",
            "Episode 015/300 | Avg Reward: 37.57 | ε: 0.797\n",
            "Episode 016/300 | Avg Reward: 38.15 | ε: 0.785\n",
            "Episode 017/300 | Avg Reward: 37.20 | ε: 0.773\n",
            "Episode 018/300 | Avg Reward: 37.26 | ε: 0.762\n",
            "Episode 019/300 | Avg Reward: 38.79 | ε: 0.750\n",
            "Episode 020/300 | Avg Reward: 37.97 | ε: 0.739\n",
            "Episode 021/300 | Avg Reward: 37.51 | ε: 0.728\n",
            "Episode 022/300 | Avg Reward: 36.83 | ε: 0.717\n",
            "Episode 023/300 | Avg Reward: 38.20 | ε: 0.706\n",
            "Episode 024/300 | Avg Reward: 37.56 | ε: 0.696\n",
            "Episode 025/300 | Avg Reward: 37.34 | ε: 0.685\n",
            "Episode 026/300 | Avg Reward: 37.81 | ε: 0.675\n",
            "Episode 027/300 | Avg Reward: 36.41 | ε: 0.665\n",
            "Episode 028/300 | Avg Reward: 38.45 | ε: 0.655\n",
            "Episode 029/300 | Avg Reward: 37.72 | ε: 0.645\n",
            "Episode 030/300 | Avg Reward: 37.02 | ε: 0.635\n",
            "Episode 031/300 | Avg Reward: 37.50 | ε: 0.626\n",
            "Episode 032/300 | Avg Reward: 37.36 | ε: 0.617\n",
            "Episode 033/300 | Avg Reward: 37.40 | ε: 0.607\n",
            "Episode 034/300 | Avg Reward: 37.30 | ε: 0.598\n",
            "Episode 035/300 | Avg Reward: 38.55 | ε: 0.589\n",
            "Episode 036/300 | Avg Reward: 37.39 | ε: 0.580\n",
            "Episode 037/300 | Avg Reward: 38.02 | ε: 0.572\n",
            "Episode 038/300 | Avg Reward: 38.78 | ε: 0.563\n",
            "Episode 039/300 | Avg Reward: 37.36 | ε: 0.555\n",
            "Episode 040/300 | Avg Reward: 38.38 | ε: 0.546\n",
            "Episode 041/300 | Avg Reward: 38.69 | ε: 0.538\n",
            "Episode 042/300 | Avg Reward: 37.10 | ε: 0.530\n",
            "Episode 043/300 | Avg Reward: 37.50 | ε: 0.522\n",
            "Episode 044/300 | Avg Reward: 37.31 | ε: 0.514\n",
            "Episode 045/300 | Avg Reward: 37.44 | ε: 0.507\n",
            "Episode 046/300 | Avg Reward: 38.43 | ε: 0.499\n",
            "Episode 047/300 | Avg Reward: 38.17 | ε: 0.491\n",
            "Episode 048/300 | Avg Reward: 38.15 | ε: 0.484\n",
            "Episode 049/300 | Avg Reward: 37.85 | ε: 0.477\n",
            "Episode 050/300 | Avg Reward: 37.50 | ε: 0.470\n",
            "Episode 051/300 | Avg Reward: 37.80 | ε: 0.463\n",
            "Episode 052/300 | Avg Reward: 37.69 | ε: 0.456\n",
            "Episode 053/300 | Avg Reward: 37.14 | ε: 0.449\n",
            "Episode 054/300 | Avg Reward: 37.61 | ε: 0.442\n",
            "Episode 055/300 | Avg Reward: 37.93 | ε: 0.436\n",
            "Episode 056/300 | Avg Reward: 36.76 | ε: 0.429\n",
            "Episode 057/300 | Avg Reward: 37.26 | ε: 0.423\n",
            "Episode 058/300 | Avg Reward: 37.97 | ε: 0.416\n",
            "Episode 059/300 | Avg Reward: 37.82 | ε: 0.410\n",
            "Episode 060/300 | Avg Reward: 36.63 | ε: 0.404\n",
            "Episode 061/300 | Avg Reward: 37.09 | ε: 0.398\n",
            "Episode 062/300 | Avg Reward: 37.93 | ε: 0.392\n",
            "Episode 063/300 | Avg Reward: 37.85 | ε: 0.386\n",
            "Episode 064/300 | Avg Reward: 37.09 | ε: 0.380\n",
            "Episode 065/300 | Avg Reward: 38.39 | ε: 0.374\n",
            "Episode 066/300 | Avg Reward: 37.53 | ε: 0.369\n",
            "Episode 067/300 | Avg Reward: 37.74 | ε: 0.363\n",
            "Episode 068/300 | Avg Reward: 37.66 | ε: 0.358\n",
            "Episode 069/300 | Avg Reward: 37.00 | ε: 0.352\n",
            "Episode 070/300 | Avg Reward: 37.53 | ε: 0.347\n",
            "Episode 071/300 | Avg Reward: 37.87 | ε: 0.342\n",
            "Episode 072/300 | Avg Reward: 37.90 | ε: 0.337\n",
            "Episode 073/300 | Avg Reward: 37.76 | ε: 0.332\n",
            "Episode 074/300 | Avg Reward: 38.31 | ε: 0.327\n",
            "Episode 075/300 | Avg Reward: 38.47 | ε: 0.322\n",
            "Episode 076/300 | Avg Reward: 36.36 | ε: 0.317\n",
            "Episode 077/300 | Avg Reward: 37.13 | ε: 0.312\n",
            "Episode 078/300 | Avg Reward: 37.72 | ε: 0.308\n",
            "Episode 079/300 | Avg Reward: 37.36 | ε: 0.303\n",
            "Episode 080/300 | Avg Reward: 36.96 | ε: 0.298\n",
            "Episode 081/300 | Avg Reward: 37.76 | ε: 0.294\n",
            "Episode 082/300 | Avg Reward: 37.96 | ε: 0.290\n",
            "Episode 083/300 | Avg Reward: 37.15 | ε: 0.285\n",
            "Episode 084/300 | Avg Reward: 37.20 | ε: 0.281\n",
            "Episode 085/300 | Avg Reward: 37.27 | ε: 0.277\n",
            "Episode 086/300 | Avg Reward: 37.83 | ε: 0.273\n",
            "Episode 087/300 | Avg Reward: 36.66 | ε: 0.269\n",
            "Episode 088/300 | Avg Reward: 36.47 | ε: 0.264\n",
            "Episode 089/300 | Avg Reward: 37.70 | ε: 0.261\n",
            "Episode 090/300 | Avg Reward: 37.95 | ε: 0.257\n",
            "Episode 091/300 | Avg Reward: 37.06 | ε: 0.253\n",
            "Episode 092/300 | Avg Reward: 37.52 | ε: 0.249\n",
            "Episode 093/300 | Avg Reward: 38.34 | ε: 0.245\n",
            "Episode 094/300 | Avg Reward: 37.93 | ε: 0.242\n",
            "Episode 095/300 | Avg Reward: 36.98 | ε: 0.238\n",
            "Episode 096/300 | Avg Reward: 37.21 | ε: 0.234\n",
            "Episode 097/300 | Avg Reward: 37.61 | ε: 0.231\n",
            "Episode 098/300 | Avg Reward: 38.39 | ε: 0.227\n",
            "Episode 099/300 | Avg Reward: 37.20 | ε: 0.224\n",
            "Episode 100/300 | Avg Reward: 37.65 | ε: 0.221\n",
            "Episode 101/300 | Avg Reward: 37.21 | ε: 0.217\n",
            "Episode 102/300 | Avg Reward: 38.04 | ε: 0.214\n",
            "Episode 103/300 | Avg Reward: 37.96 | ε: 0.211\n",
            "Episode 104/300 | Avg Reward: 37.72 | ε: 0.208\n",
            "Episode 105/300 | Avg Reward: 37.35 | ε: 0.205\n",
            "Episode 106/300 | Avg Reward: 37.75 | ε: 0.201\n",
            "Episode 107/300 | Avg Reward: 37.35 | ε: 0.198\n",
            "Episode 108/300 | Avg Reward: 37.99 | ε: 0.195\n",
            "Episode 109/300 | Avg Reward: 36.56 | ε: 0.193\n",
            "Episode 110/300 | Avg Reward: 37.88 | ε: 0.190\n",
            "Episode 111/300 | Avg Reward: 37.88 | ε: 0.187\n",
            "Episode 112/300 | Avg Reward: 38.09 | ε: 0.184\n",
            "Episode 113/300 | Avg Reward: 37.45 | ε: 0.181\n",
            "Episode 114/300 | Avg Reward: 36.45 | ε: 0.179\n",
            "Episode 115/300 | Avg Reward: 39.18 | ε: 0.176\n",
            "Episode 116/300 | Avg Reward: 37.67 | ε: 0.173\n",
            "Episode 117/300 | Avg Reward: 37.83 | ε: 0.171\n",
            "Episode 118/300 | Avg Reward: 37.70 | ε: 0.168\n",
            "Episode 119/300 | Avg Reward: 39.27 | ε: 0.166\n",
            "Episode 120/300 | Avg Reward: 37.89 | ε: 0.163\n",
            "Episode 121/300 | Avg Reward: 38.10 | ε: 0.161\n",
            "Episode 122/300 | Avg Reward: 37.40 | ε: 0.158\n",
            "Episode 123/300 | Avg Reward: 37.36 | ε: 0.156\n",
            "Episode 124/300 | Avg Reward: 37.52 | ε: 0.153\n",
            "Episode 125/300 | Avg Reward: 37.68 | ε: 0.151\n",
            "Episode 126/300 | Avg Reward: 38.00 | ε: 0.150\n",
            "Episode 127/300 | Avg Reward: 38.25 | ε: 0.150\n",
            "Episode 128/300 | Avg Reward: 38.63 | ε: 0.150\n",
            "Episode 129/300 | Avg Reward: 37.93 | ε: 0.150\n",
            "Episode 130/300 | Avg Reward: 36.94 | ε: 0.150\n",
            "Episode 131/300 | Avg Reward: 38.03 | ε: 0.150\n",
            "Episode 132/300 | Avg Reward: 37.67 | ε: 0.150\n",
            "Episode 133/300 | Avg Reward: 37.49 | ε: 0.150\n",
            "Episode 134/300 | Avg Reward: 37.77 | ε: 0.150\n",
            "Episode 135/300 | Avg Reward: 38.13 | ε: 0.150\n",
            "Episode 136/300 | Avg Reward: 38.16 | ε: 0.150\n",
            "Episode 137/300 | Avg Reward: 36.65 | ε: 0.150\n",
            "Episode 138/300 | Avg Reward: 37.86 | ε: 0.150\n",
            "Episode 139/300 | Avg Reward: 38.10 | ε: 0.150\n",
            "Episode 140/300 | Avg Reward: 36.46 | ε: 0.150\n",
            "Episode 141/300 | Avg Reward: 36.97 | ε: 0.150\n",
            "Episode 142/300 | Avg Reward: 38.05 | ε: 0.150\n",
            "Episode 143/300 | Avg Reward: 37.59 | ε: 0.150\n",
            "Episode 144/300 | Avg Reward: 37.99 | ε: 0.150\n",
            "Episode 145/300 | Avg Reward: 37.00 | ε: 0.150\n",
            "Episode 146/300 | Avg Reward: 37.71 | ε: 0.150\n",
            "Episode 147/300 | Avg Reward: 37.93 | ε: 0.150\n",
            "Episode 148/300 | Avg Reward: 37.95 | ε: 0.150\n",
            "Episode 149/300 | Avg Reward: 37.98 | ε: 0.150\n",
            "Episode 150/300 | Avg Reward: 38.34 | ε: 0.150\n",
            "Episode 151/300 | Avg Reward: 37.35 | ε: 0.150\n",
            "Episode 152/300 | Avg Reward: 37.40 | ε: 0.150\n",
            "Episode 153/300 | Avg Reward: 37.32 | ε: 0.150\n",
            "Episode 154/300 | Avg Reward: 37.71 | ε: 0.150\n",
            "Episode 155/300 | Avg Reward: 38.13 | ε: 0.150\n",
            "Episode 156/300 | Avg Reward: 38.39 | ε: 0.150\n",
            "Episode 157/300 | Avg Reward: 38.51 | ε: 0.150\n",
            "Episode 158/300 | Avg Reward: 37.29 | ε: 0.150\n",
            "Episode 159/300 | Avg Reward: 37.38 | ε: 0.150\n",
            "Episode 160/300 | Avg Reward: 37.65 | ε: 0.150\n",
            "Episode 161/300 | Avg Reward: 37.16 | ε: 0.150\n",
            "Episode 162/300 | Avg Reward: 37.78 | ε: 0.150\n",
            "Episode 163/300 | Avg Reward: 37.20 | ε: 0.150\n",
            "Episode 164/300 | Avg Reward: 37.82 | ε: 0.150\n",
            "Episode 165/300 | Avg Reward: 37.62 | ε: 0.150\n",
            "Episode 166/300 | Avg Reward: 37.49 | ε: 0.150\n",
            "Episode 167/300 | Avg Reward: 37.68 | ε: 0.150\n",
            "Episode 168/300 | Avg Reward: 37.55 | ε: 0.150\n",
            "Episode 169/300 | Avg Reward: 37.77 | ε: 0.150\n",
            "Episode 170/300 | Avg Reward: 37.95 | ε: 0.150\n",
            "Episode 171/300 | Avg Reward: 37.32 | ε: 0.150\n",
            "Episode 172/300 | Avg Reward: 37.90 | ε: 0.150\n",
            "Episode 173/300 | Avg Reward: 38.43 | ε: 0.150\n",
            "Episode 174/300 | Avg Reward: 38.04 | ε: 0.150\n",
            "Episode 175/300 | Avg Reward: 37.00 | ε: 0.150\n",
            "Episode 176/300 | Avg Reward: 37.72 | ε: 0.150\n",
            "Episode 177/300 | Avg Reward: 37.93 | ε: 0.150\n",
            "Episode 178/300 | Avg Reward: 37.82 | ε: 0.150\n",
            "Episode 179/300 | Avg Reward: 38.56 | ε: 0.150\n",
            "Episode 180/300 | Avg Reward: 37.35 | ε: 0.150\n",
            "Episode 181/300 | Avg Reward: 38.33 | ε: 0.150\n",
            "Episode 182/300 | Avg Reward: 37.21 | ε: 0.150\n",
            "Episode 183/300 | Avg Reward: 37.93 | ε: 0.150\n",
            "Episode 184/300 | Avg Reward: 38.22 | ε: 0.150\n",
            "Episode 185/300 | Avg Reward: 38.34 | ε: 0.150\n",
            "Episode 186/300 | Avg Reward: 37.83 | ε: 0.150\n",
            "Episode 187/300 | Avg Reward: 37.56 | ε: 0.150\n",
            "Episode 188/300 | Avg Reward: 37.21 | ε: 0.150\n",
            "Episode 189/300 | Avg Reward: 37.55 | ε: 0.150\n",
            "Episode 190/300 | Avg Reward: 36.86 | ε: 0.150\n",
            "Episode 191/300 | Avg Reward: 37.69 | ε: 0.150\n",
            "Episode 192/300 | Avg Reward: 37.72 | ε: 0.150\n",
            "Episode 193/300 | Avg Reward: 37.66 | ε: 0.150\n",
            "Episode 194/300 | Avg Reward: 38.61 | ε: 0.150\n",
            "Episode 195/300 | Avg Reward: 37.68 | ε: 0.150\n",
            "Episode 196/300 | Avg Reward: 37.93 | ε: 0.150\n",
            "Episode 197/300 | Avg Reward: 38.22 | ε: 0.150\n",
            "Episode 198/300 | Avg Reward: 37.34 | ε: 0.150\n",
            "Episode 199/300 | Avg Reward: 37.72 | ε: 0.150\n",
            "Episode 200/300 | Avg Reward: 38.28 | ε: 0.150\n",
            "Episode 201/300 | Avg Reward: 38.17 | ε: 0.150\n",
            "Episode 202/300 | Avg Reward: 37.77 | ε: 0.150\n",
            "Episode 203/300 | Avg Reward: 38.50 | ε: 0.150\n",
            "Episode 204/300 | Avg Reward: 37.25 | ε: 0.150\n",
            "Episode 205/300 | Avg Reward: 37.87 | ε: 0.150\n",
            "Episode 206/300 | Avg Reward: 38.26 | ε: 0.150\n",
            "Episode 207/300 | Avg Reward: 37.87 | ε: 0.150\n",
            "Episode 208/300 | Avg Reward: 37.99 | ε: 0.150\n",
            "Episode 209/300 | Avg Reward: 37.31 | ε: 0.150\n",
            "Episode 210/300 | Avg Reward: 37.50 | ε: 0.150\n",
            "Episode 211/300 | Avg Reward: 38.81 | ε: 0.150\n",
            "Episode 212/300 | Avg Reward: 37.38 | ε: 0.150\n",
            "Episode 213/300 | Avg Reward: 38.28 | ε: 0.150\n",
            "Episode 214/300 | Avg Reward: 38.49 | ε: 0.150\n",
            "Episode 215/300 | Avg Reward: 37.80 | ε: 0.150\n",
            "Episode 216/300 | Avg Reward: 37.20 | ε: 0.150\n",
            "Episode 217/300 | Avg Reward: 37.59 | ε: 0.150\n",
            "Episode 218/300 | Avg Reward: 36.66 | ε: 0.150\n",
            "Episode 219/300 | Avg Reward: 37.42 | ε: 0.150\n",
            "Episode 220/300 | Avg Reward: 38.63 | ε: 0.150\n",
            "Episode 221/300 | Avg Reward: 37.39 | ε: 0.150\n",
            "Episode 222/300 | Avg Reward: 37.60 | ε: 0.150\n",
            "Episode 223/300 | Avg Reward: 37.92 | ε: 0.150\n",
            "Episode 224/300 | Avg Reward: 37.49 | ε: 0.150\n",
            "Episode 225/300 | Avg Reward: 37.54 | ε: 0.150\n",
            "Episode 226/300 | Avg Reward: 37.46 | ε: 0.150\n",
            "Episode 227/300 | Avg Reward: 37.90 | ε: 0.150\n",
            "Episode 228/300 | Avg Reward: 37.55 | ε: 0.150\n",
            "Episode 229/300 | Avg Reward: 37.94 | ε: 0.150\n",
            "Episode 230/300 | Avg Reward: 38.10 | ε: 0.150\n",
            "Episode 231/300 | Avg Reward: 36.32 | ε: 0.150\n",
            "Episode 232/300 | Avg Reward: 36.39 | ε: 0.150\n",
            "Episode 233/300 | Avg Reward: 38.02 | ε: 0.150\n",
            "Episode 234/300 | Avg Reward: 37.88 | ε: 0.150\n",
            "Episode 235/300 | Avg Reward: 37.61 | ε: 0.150\n",
            "Episode 236/300 | Avg Reward: 37.68 | ε: 0.150\n",
            "Episode 237/300 | Avg Reward: 37.70 | ε: 0.150\n",
            "Episode 238/300 | Avg Reward: 36.96 | ε: 0.150\n",
            "Episode 239/300 | Avg Reward: 37.68 | ε: 0.150\n",
            "Episode 240/300 | Avg Reward: 37.89 | ε: 0.150\n",
            "Episode 241/300 | Avg Reward: 36.77 | ε: 0.150\n",
            "Episode 242/300 | Avg Reward: 38.14 | ε: 0.150\n",
            "Episode 243/300 | Avg Reward: 36.96 | ε: 0.150\n",
            "Episode 244/300 | Avg Reward: 37.78 | ε: 0.150\n",
            "Episode 245/300 | Avg Reward: 38.69 | ε: 0.150\n",
            "Episode 246/300 | Avg Reward: 36.67 | ε: 0.150\n",
            "Episode 247/300 | Avg Reward: 37.55 | ε: 0.150\n",
            "Episode 248/300 | Avg Reward: 38.07 | ε: 0.150\n",
            "Episode 249/300 | Avg Reward: 38.32 | ε: 0.150\n",
            "Episode 250/300 | Avg Reward: 37.30 | ε: 0.150\n",
            "Episode 251/300 | Avg Reward: 37.99 | ε: 0.150\n",
            "Episode 252/300 | Avg Reward: 37.18 | ε: 0.150\n",
            "Episode 253/300 | Avg Reward: 37.18 | ε: 0.150\n",
            "Episode 254/300 | Avg Reward: 38.01 | ε: 0.150\n",
            "Episode 255/300 | Avg Reward: 38.06 | ε: 0.150\n",
            "Episode 256/300 | Avg Reward: 37.58 | ε: 0.150\n",
            "Episode 257/300 | Avg Reward: 37.58 | ε: 0.150\n",
            "Episode 258/300 | Avg Reward: 38.24 | ε: 0.150\n",
            "Episode 259/300 | Avg Reward: 37.59 | ε: 0.150\n",
            "Episode 260/300 | Avg Reward: 37.72 | ε: 0.150\n",
            "Episode 261/300 | Avg Reward: 37.43 | ε: 0.150\n",
            "Episode 262/300 | Avg Reward: 38.15 | ε: 0.150\n",
            "Episode 263/300 | Avg Reward: 38.68 | ε: 0.150\n",
            "Episode 264/300 | Avg Reward: 38.27 | ε: 0.150\n",
            "Episode 265/300 | Avg Reward: 37.94 | ε: 0.150\n",
            "Episode 266/300 | Avg Reward: 37.75 | ε: 0.150\n",
            "Episode 267/300 | Avg Reward: 37.54 | ε: 0.150\n",
            "Episode 268/300 | Avg Reward: 37.49 | ε: 0.150\n",
            "Episode 269/300 | Avg Reward: 37.87 | ε: 0.150\n",
            "Episode 270/300 | Avg Reward: 37.36 | ε: 0.150\n",
            "Episode 271/300 | Avg Reward: 37.43 | ε: 0.150\n",
            "Episode 272/300 | Avg Reward: 37.13 | ε: 0.150\n",
            "Episode 273/300 | Avg Reward: 36.71 | ε: 0.150\n",
            "Episode 274/300 | Avg Reward: 37.55 | ε: 0.150\n",
            "Episode 275/300 | Avg Reward: 36.72 | ε: 0.150\n",
            "Episode 276/300 | Avg Reward: 37.89 | ε: 0.150\n",
            "Episode 277/300 | Avg Reward: 36.93 | ε: 0.150\n",
            "Episode 278/300 | Avg Reward: 37.36 | ε: 0.150\n",
            "Episode 279/300 | Avg Reward: 38.33 | ε: 0.150\n",
            "Episode 280/300 | Avg Reward: 38.06 | ε: 0.150\n",
            "Episode 281/300 | Avg Reward: 36.94 | ε: 0.150\n",
            "Episode 282/300 | Avg Reward: 37.28 | ε: 0.150\n",
            "Episode 283/300 | Avg Reward: 36.96 | ε: 0.150\n",
            "Episode 284/300 | Avg Reward: 37.55 | ε: 0.150\n",
            "Episode 285/300 | Avg Reward: 37.82 | ε: 0.150\n",
            "Episode 286/300 | Avg Reward: 37.80 | ε: 0.150\n",
            "Episode 287/300 | Avg Reward: 38.38 | ε: 0.150\n",
            "Episode 288/300 | Avg Reward: 38.06 | ε: 0.150\n",
            "Episode 289/300 | Avg Reward: 37.28 | ε: 0.150\n",
            "Episode 290/300 | Avg Reward: 37.57 | ε: 0.150\n",
            "Episode 291/300 | Avg Reward: 38.95 | ε: 0.150\n",
            "Episode 292/300 | Avg Reward: 37.71 | ε: 0.150\n",
            "Episode 293/300 | Avg Reward: 37.85 | ε: 0.150\n",
            "Episode 294/300 | Avg Reward: 37.48 | ε: 0.150\n",
            "Episode 295/300 | Avg Reward: 37.57 | ε: 0.150\n",
            "Episode 296/300 | Avg Reward: 36.71 | ε: 0.150\n",
            "Episode 297/300 | Avg Reward: 38.59 | ε: 0.150\n",
            "Episode 298/300 | Avg Reward: 38.28 | ε: 0.150\n",
            "Episode 299/300 | Avg Reward: 36.78 | ε: 0.150\n",
            "Episode 300/300 | Avg Reward: 37.04 | ε: 0.150\n",
            "Training completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION WITH TEST_DATA"
      ],
      "metadata": {
        "id": "k-2G7dH3NAzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "PROMOTION_IDS = ['PR100', 'PR101', 'PR102', 'PR103', 'PR104',\n",
        "                'PR105', 'PR106', 'PR107', 'PR108', 'PR109']\n",
        "\n",
        "class AdvancedRewardSystem:\n",
        "    def __init__(self, config):\n",
        "        self.weights = config['weights']\n",
        "        self.segment_multipliers = config['segment_multipliers']\n",
        "        self.promo_costs = {\n",
        "            'PR100': 0.15, 'PR101': 0.20, 'PR102': 0.18,\n",
        "            'PR103': 0.25, 'PR104': 0.22, 'PR105': 0.19,\n",
        "            'PR106': 0.17, 'PR107': 0.21, 'PR108': 0.23,\n",
        "            'PR109': 0.16\n",
        "        }\n",
        "        self.eligibility_map = {\n",
        "            'Low': ['PR100', 'PR101', 'PR102'],\n",
        "            'Medium': ['PR103', 'PR104', 'PR105'],\n",
        "            'High': ['PR106', 'PR107', 'PR108', 'PR109']\n",
        "        }\n",
        "\n",
        "    def get_valid_actions(self, promotion_history):\n",
        "        eligible = self.eligibility_map.get(promotion_history, [])\n",
        "        return [PROMOTION_IDS.index(p) for p in eligible if p in PROMOTION_IDS]\n",
        "\n",
        "    def calculate_reward(self, transaction, action):\n",
        "        promo_id = PROMOTION_IDS[action]\n",
        "        cost = self.promo_costs[promo_id]\n",
        "\n",
        "        response = transaction['response']\n",
        "        quantity = transaction['quantity']\n",
        "        price = transaction['price']\n",
        "        margin = transaction['margin'] - cost\n",
        "        loyalty = transaction['loyalty_score']\n",
        "        churn = transaction['churn_score']\n",
        "        segment = transaction.get('segment', 'Standard')\n",
        "\n",
        "        sales_reward = math.log1p(quantity * price) * response * 2.0\n",
        "        margin_reward = margin * quantity * 0.5\n",
        "        retention_reward = (1 / (1 + math.exp(-loyalty))) - math.exp(churn)\n",
        "        penalty = (1 - response) * price * 0.3\n",
        "\n",
        "        segment_mult = self.segment_multipliers.get(segment, 1.0)\n",
        "        return ((self.weights['sales'] * sales_reward +\n",
        "                self.weights['margin'] * margin_reward +\n",
        "                self.weights['retention'] * retention_reward -\n",
        "                penalty) * segment_mult)\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = len(PROMOTION_IDS)\n",
        "        self.memory = deque(maxlen=20000)\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.985\n",
        "        self.epsilon_min = 0.15\n",
        "        self.batch_size = 256\n",
        "\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.update_target()\n",
        "        self.optimizer = optim.AdamW(self.model.parameters(), lr=0.001)\n",
        "        self.loss_fn = nn.SmoothL1Loss()\n",
        "\n",
        "    def _build_model(self):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(self.state_size, 512),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(256, self.action_size)\n",
        "        )\n",
        "\n",
        "    def update_target(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def act(self, state, valid_actions):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.choice(valid_actions)\n",
        "\n",
        "        state = torch.FloatTensor(state)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.model(state)\n",
        "\n",
        "        valid_q = q_values[valid_actions]\n",
        "        return valid_actions[torch.argmax(valid_q).item()]\n",
        "\n",
        "def preprocess_data(df, train_stats=None):\n",
        "    df = df.drop(['user_id', 'product_id', 'promotion_used'], axis=1, errors='ignore')\n",
        "    categorical_cols = ['category', 'gender', 'segment', 'promotion_history']\n",
        "    df = pd.get_dummies(df, columns=categorical_cols)\n",
        "    numeric_cols = ['quantity', 'price', 'margin', 'churn_score',\n",
        "                   'loyalty_score', 'avg_order_value', 'age', 'income']\n",
        "\n",
        "    if train_stats:\n",
        "        means = train_stats['means']\n",
        "        stds = train_stats['stds']\n",
        "    else:\n",
        "        means = df[numeric_cols].mean()\n",
        "        stds = df[numeric_cols].std()\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = (df[col] - means[col]) / stds[col]\n",
        "\n",
        "    return df.fillna(0), (means, stds) if train_stats is None else None\n",
        "\n",
        "def evaluate_model(test_path, model_path, reward_config, train_stats):\n",
        "    df_test = pd.read_csv(test_path)\n",
        "    df_test = df_test[df_test['promotion_used'].isin(PROMOTION_IDS)].copy()\n",
        "    processed_test, _ = preprocess_data(df_test, train_stats)\n",
        "    state_columns = [col for col in processed_test.columns if col != 'action']\n",
        "    states_test = processed_test[state_columns].values.astype(np.float32)\n",
        "    states_test = np.nan_to_num(states_test)\n",
        "    reward_system = AdvancedRewardSystem(reward_config)\n",
        "    agent = DQNAgent(states_test.shape[1])\n",
        "    agent.model.load_state_dict(torch.load(model_path))\n",
        "    agent.model.eval()\n",
        "    agent.epsilon = 0.0\n",
        "    total_reward = 0\n",
        "    correct_predictions = 0\n",
        "    action_counts = {p: 0 for p in PROMOTION_IDS}\n",
        "\n",
        "    for idx in range(len(df_test)):\n",
        "        row = df_test.iloc[idx]\n",
        "        state = states_test[idx]\n",
        "        valid_actions = reward_system.get_valid_actions(row['promotion_history'])\n",
        "        if not valid_actions:\n",
        "            continue\n",
        "        action = agent.act(state, valid_actions)\n",
        "        promo_used = PROMOTION_IDS[action]\n",
        "        action_counts[promo_used] += 1\n",
        "        reward = reward_system.calculate_reward(row.to_dict(), action)\n",
        "        total_reward += reward\n",
        "        if promo_used == row['promotion_used']:\n",
        "            correct_predictions += 1\n",
        "    avg_reward = total_reward / len(df_test) if len(df_test) > 0 else 0\n",
        "    accuracy = (correct_predictions / len(df_test)) * 100 if len(df_test) > 0 else 0\n",
        "\n",
        "    print(f\"\\n{' Evaluation Results ':=^40}\")\n",
        "    print(f\"Test Samples: {len(df_test)}\")\n",
        "    print(f\"Total Reward: {total_reward:.2f}\")\n",
        "    print(f\"Avg Reward/Sample: {avg_reward:.2f}\")\n",
        "    print(f\"Accuracy vs Historical: {accuracy:.2f}%\")\n",
        "\n",
        "    print(\"\\nAction Distribution:\")\n",
        "    for promo, count in action_counts.items():\n",
        "        print(f\"{promo}: {count} ({count/len(df_test)*100:.1f}%)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_df = pd.read_csv('train_data.csv')\n",
        "    _, train_stats = preprocess_data(train_df)\n",
        "\n",
        "    reward_config = {\n",
        "        'weights': {'sales': 0.4, 'margin': 0.35, 'retention': 0.25},\n",
        "        'segment_multipliers': {'Premium': 1.6, 'Standard': 1.1, 'Budget': 0.7}\n",
        "    }\n",
        "\n",
        "    evaluate_model(\n",
        "        test_path='test_data.csv',\n",
        "        model_path='/content/final_promotion_model.pth',\n",
        "        reward_config=reward_config,\n",
        "        train_stats={'means': train_stats[0], 'stds': train_stats[1]}\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEC2mU3GIMVv",
        "outputId": "09155e9d-4e23-4e20-bceb-39a1a37abfe3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Evaluation Results ==========\n",
            "Test Samples: 43\n",
            "Total Reward: 1376.91\n",
            "Avg Reward/Sample: 32.02\n",
            "Accuracy vs Historical: 16.28%\n",
            "\n",
            "Action Distribution:\n",
            "PR100: 0 (0.0%)\n",
            "PR101: 0 (0.0%)\n",
            "PR102: 16 (37.2%)\n",
            "PR103: 9 (20.9%)\n",
            "PR104: 0 (0.0%)\n",
            "PR105: 0 (0.0%)\n",
            "PR106: 16 (37.2%)\n",
            "PR107: 0 (0.0%)\n",
            "PR108: 1 (2.3%)\n",
            "PR109: 1 (2.3%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RL AGENT TRAINING (PPO ALGORITHM)"
      ],
      "metadata": {
        "id": "rn1cCdifNJvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import deque\n",
        "import random\n",
        "import math\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "PROMOTION_IDS = ['PR100', 'PR101', 'PR102', 'PR103', 'PR104',\n",
        "                'PR105', 'PR106', 'PR107', 'PR108', 'PR109']\n",
        "ACTION_SPACE_SIZE = len(PROMOTION_IDS)\n",
        "\n",
        "class AdvancedRewardSystem:\n",
        "    def __init__(self, config):\n",
        "        self.weights = config['weights']\n",
        "        self.segment_multipliers = config['segment_multipliers']\n",
        "        self.promo_costs = {\n",
        "            'PR100': 0.15, 'PR101': 0.20, 'PR102': 0.18,\n",
        "            'PR103': 0.25, 'PR104': 0.22, 'PR105': 0.19,\n",
        "            'PR106': 0.17, 'PR107': 0.21, 'PR108': 0.23,\n",
        "            'PR109': 0.16\n",
        "        }\n",
        "        self.eligibility_map = {\n",
        "            'Low': ['PR100', 'PR101', 'PR102'],\n",
        "            'Medium': ['PR103', 'PR104', 'PR105'],\n",
        "            'High': ['PR106', 'PR107', 'PR108', 'PR109']\n",
        "        }\n",
        "\n",
        "    def get_valid_actions(self, promotion_history):\n",
        "        eligible = self.eligibility_map.get(promotion_history, [])\n",
        "        return [PROMOTION_IDS.index(p) for p in eligible if p in PROMOTION_IDS]\n",
        "\n",
        "    def calculate_reward(self, transaction, action):\n",
        "        promo_id = PROMOTION_IDS[action]\n",
        "        cost = self.promo_costs[promo_id]\n",
        "\n",
        "        response = transaction['response']\n",
        "        quantity = transaction['quantity']\n",
        "        price = transaction['price']\n",
        "        margin = transaction['margin'] - cost\n",
        "        loyalty = transaction['loyalty_score']\n",
        "        churn = transaction['churn_score']\n",
        "        segment = transaction.get('segment', 'Standard')\n",
        "        sales_reward = math.log1p(quantity * price) * response * 2.0\n",
        "        margin_reward = margin * quantity * 0.5\n",
        "        retention_reward = (1 / (1 + math.exp(-loyalty))) - math.exp(churn)\n",
        "        penalty = (1 - response) * price * 0.3\n",
        "        segment_mult = self.segment_multipliers.get(segment, 1.0)\n",
        "\n",
        "        return ((self.weights['sales'] * sales_reward +\n",
        "                self.weights['margin'] * margin_reward +\n",
        "                self.weights['retention'] * retention_reward -\n",
        "                penalty) * segment_mult)\n",
        "\n",
        "class RolloutBuffer:\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.log_probs = []\n",
        "        self.rewards = []\n",
        "        self.values = []\n",
        "        self.dones = []\n",
        "        self.advantages = []\n",
        "        self.returns = []\n",
        "        self.valid_actions_masks = []\n",
        "\n",
        "    def add(self, state, action, log_prob, reward, value, done, valid_actions):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.log_probs.append(log_prob)\n",
        "        self.rewards.append(reward)\n",
        "        self.values.append(value)\n",
        "        self.dones.append(done)\n",
        "        self.valid_actions_masks.append(valid_actions)\n",
        "\n",
        "    def clear(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.log_probs = []\n",
        "        self.rewards = []\n",
        "        self.values = []\n",
        "        self.dones = []\n",
        "        self.advantages = []\n",
        "        self.returns = []\n",
        "        self.valid_actions_masks = []\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, state_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = ACTION_SPACE_SIZE\n",
        "        self.gamma = 0.99\n",
        "        self.gae_lambda = 0.95\n",
        "        self.eps_clip = 0.2\n",
        "        self.entropy_coeff = 0.01\n",
        "        self.critic_coeff = 0.5\n",
        "        self.batch_size = 64\n",
        "        self.n_epochs = 10\n",
        "        self.actor = self._build_actor()\n",
        "        self.critic = self._build_critic()\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "        self.buffer = RolloutBuffer()\n",
        "\n",
        "    def _build_actor(self):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(self.state_size, 512),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(256, self.action_size)\n",
        "        )\n",
        "\n",
        "    def _build_critic(self):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(self.state_size, 512),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def act(self, state, valid_actions):\n",
        "        state = torch.FloatTensor(state)\n",
        "        with torch.no_grad():\n",
        "            logits = self.actor(state)\n",
        "            valid_actions_tensor = torch.tensor(valid_actions, dtype=torch.long)\n",
        "            masked_logits = torch.full_like(logits, -1e8)\n",
        "            masked_logits[valid_actions_tensor] = logits[valid_actions_tensor]\n",
        "\n",
        "            dist = Categorical(logits=masked_logits)\n",
        "            action = dist.sample()\n",
        "            log_prob = dist.log_prob(action)\n",
        "            value = self.critic(state)\n",
        "\n",
        "        return action.item(), log_prob.item(), value.item()\n",
        "\n",
        "    def evaluate(self, states, actions, valid_actions_masks):\n",
        "        if valid_actions_masks is None:\n",
        "            valid_actions_masks = torch.ones_like(states[:, :self.action_size])\n",
        "        logits = self.actor(states)\n",
        "        masked_logits = logits * valid_actions_masks + (1 - valid_actions_masks) * -1e8\n",
        "        dist = Categorical(logits=masked_logits)\n",
        "        log_probs = dist.log_prob(actions)\n",
        "        entropy = dist.entropy().mean()\n",
        "        values = self.critic(states).squeeze()\n",
        "        return log_probs, values, entropy\n",
        "\n",
        "    def compute_gae(self):\n",
        "        rewards = torch.tensor(self.buffer.rewards, dtype=torch.float32)\n",
        "        values = torch.tensor(self.buffer.values, dtype=torch.float32)\n",
        "        dones = torch.tensor(self.buffer.dones, dtype=torch.float32)\n",
        "        values = torch.cat([values, torch.zeros(1)])\n",
        "        advantages = torch.zeros_like(rewards)\n",
        "        gae = 0\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            delta = rewards[t] + self.gamma * values[t+1] * (1 - dones[t]) - values[t]\n",
        "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
        "            advantages[t] = gae\n",
        "        returns = advantages + values[:-1]\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "        return advantages, returns\n",
        "\n",
        "    def update(self):\n",
        "        states_tensor = torch.FloatTensor(np.array(self.buffer.states))\n",
        "        actions_tensor = torch.LongTensor(self.buffer.actions)\n",
        "        old_log_probs_tensor = torch.FloatTensor(self.buffer.log_probs)\n",
        "        valid_actions_masks_tensor = torch.zeros(len(self.buffer.valid_actions_masks), ACTION_SPACE_SIZE)\n",
        "        for i, valid_actions in enumerate(self.buffer.valid_actions_masks):\n",
        "            valid_actions_masks_tensor[i, valid_actions] = 1\n",
        "        advantages_tensor, returns_tensor = self.compute_gae()\n",
        "        dataset = TensorDataset(\n",
        "            states_tensor,\n",
        "            actions_tensor,\n",
        "            old_log_probs_tensor,\n",
        "            advantages_tensor,\n",
        "            returns_tensor,\n",
        "            valid_actions_masks_tensor\n",
        "        )\n",
        "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        for _ in range(self.n_epochs):\n",
        "            for batch in loader:\n",
        "                states, actions, old_log_probs, advantages, returns, valid_actions_masks = batch\n",
        "                new_log_probs, values, entropy = self.evaluate(states, actions, valid_actions_masks)\n",
        "                ratios = torch.exp(new_log_probs - old_log_probs)\n",
        "                surr1 = ratios * advantages\n",
        "                surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
        "                actor_loss = -torch.min(surr1, surr2).mean()\n",
        "                critic_loss = torch.nn.functional.mse_loss(values, returns)\n",
        "                total_loss = (\n",
        "                    actor_loss +\n",
        "                    self.critic_coeff * critic_loss -\n",
        "                    self.entropy_coeff * entropy\n",
        "                )\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)\n",
        "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n",
        "                self.actor_optimizer.step()\n",
        "                self.critic_optimizer.step()\n",
        "        self.buffer.clear()\n",
        "\n",
        "def preprocess_data(df):\n",
        "    df = df.drop(['user_id', 'product_id', 'promotion_used'], axis=1, errors='ignore')\n",
        "    categorical_cols = ['category', 'gender', 'segment', 'promotion_history']\n",
        "    df = pd.get_dummies(df, columns=categorical_cols)\n",
        "    numeric_cols = ['quantity', 'price', 'margin', 'churn_score',\n",
        "                   'loyalty_score', 'avg_order_value', 'age', 'income']\n",
        "    for col in numeric_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = (df[col] - df[col].mean()) / df[col].std()\n",
        "\n",
        "    return df.fillna(0)\n",
        "\n",
        "def train_ppo_agent(df, config):\n",
        "    df = df[df['promotion_used'].isin(PROMOTION_IDS)].copy()\n",
        "    processed_df = preprocess_data(df)\n",
        "    promotion_map = {p: i for i, p in enumerate(PROMOTION_IDS)}\n",
        "    processed_df['action'] = df['promotion_used'].map(promotion_map)\n",
        "    state_columns = [col for col in processed_df.columns if col != 'action']\n",
        "    states = processed_df[state_columns].values.astype(np.float32)\n",
        "    states = np.nan_to_num(states)\n",
        "    reward_system = AdvancedRewardSystem(config)\n",
        "    agent = PPOAgent(states.shape[1])\n",
        "    episodes = 300\n",
        "    steps_per_episode = 300\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        total_reward = 0\n",
        "        indices = np.random.permutation(len(processed_df))\n",
        "\n",
        "        for step in range(steps_per_episode):\n",
        "            idx = indices[step % len(processed_df)]\n",
        "            row = df.iloc[idx]\n",
        "            state = states[idx]\n",
        "            valid_actions = reward_system.get_valid_actions(row['promotion_history'])\n",
        "            if not valid_actions:\n",
        "                continue\n",
        "            action, log_prob, value = agent.act(state, valid_actions)\n",
        "            reward = reward_system.calculate_reward(row.to_dict(), action)\n",
        "            next_idx = (idx + 1) % len(processed_df)\n",
        "            next_state = states[next_idx]\n",
        "            agent.buffer.add(\n",
        "                state, action, log_prob, reward, value,\n",
        "                False, valid_actions\n",
        "            )\n",
        "\n",
        "            total_reward += reward\n",
        "        agent.update()\n",
        "\n",
        "        avg_reward = total_reward / steps_per_episode\n",
        "        print(f\"Episode {episode+1:03d}/{episodes} | Avg Reward: {avg_reward:.2f}\")\n",
        "\n",
        "    return agent\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = pd.read_csv('train_data.csv')\n",
        "\n",
        "    reward_config = {\n",
        "        'weights': {\n",
        "            'sales': 0.4,\n",
        "            'margin': 0.35,\n",
        "            'retention': 0.25\n",
        "        },\n",
        "        'segment_multipliers': {\n",
        "            'Premium': 1.6,\n",
        "            'Standard': 1.1,\n",
        "            'Budget': 0.7\n",
        "        }\n",
        "    }\n",
        "\n",
        "    trained_agent = train_ppo_agent(df, reward_config)\n",
        "    torch.save(trained_agent.actor.state_dict(), 'final_promotion_model.pth')\n",
        "    print(\"Training completed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xhs9R-NVNTAZ",
        "outputId": "d1a06806-6885-4b5f-ac64-4460a5588487"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 001/300 | Avg Reward: 37.23\n",
            "Episode 002/300 | Avg Reward: 36.94\n",
            "Episode 003/300 | Avg Reward: 37.75\n",
            "Episode 004/300 | Avg Reward: 37.87\n",
            "Episode 005/300 | Avg Reward: 37.40\n",
            "Episode 006/300 | Avg Reward: 37.69\n",
            "Episode 007/300 | Avg Reward: 37.72\n",
            "Episode 008/300 | Avg Reward: 38.06\n",
            "Episode 009/300 | Avg Reward: 38.09\n",
            "Episode 010/300 | Avg Reward: 37.76\n",
            "Episode 011/300 | Avg Reward: 37.96\n",
            "Episode 012/300 | Avg Reward: 37.75\n",
            "Episode 013/300 | Avg Reward: 37.95\n",
            "Episode 014/300 | Avg Reward: 36.80\n",
            "Episode 015/300 | Avg Reward: 37.28\n",
            "Episode 016/300 | Avg Reward: 38.09\n",
            "Episode 017/300 | Avg Reward: 37.13\n",
            "Episode 018/300 | Avg Reward: 38.05\n",
            "Episode 019/300 | Avg Reward: 37.57\n",
            "Episode 020/300 | Avg Reward: 38.43\n",
            "Episode 021/300 | Avg Reward: 36.90\n",
            "Episode 022/300 | Avg Reward: 36.42\n",
            "Episode 023/300 | Avg Reward: 37.57\n",
            "Episode 024/300 | Avg Reward: 38.03\n",
            "Episode 025/300 | Avg Reward: 38.58\n",
            "Episode 026/300 | Avg Reward: 35.87\n",
            "Episode 027/300 | Avg Reward: 38.01\n",
            "Episode 028/300 | Avg Reward: 36.77\n",
            "Episode 029/300 | Avg Reward: 37.70\n",
            "Episode 030/300 | Avg Reward: 37.26\n",
            "Episode 031/300 | Avg Reward: 38.22\n",
            "Episode 032/300 | Avg Reward: 37.57\n",
            "Episode 033/300 | Avg Reward: 37.55\n",
            "Episode 034/300 | Avg Reward: 37.10\n",
            "Episode 035/300 | Avg Reward: 37.83\n",
            "Episode 036/300 | Avg Reward: 37.30\n",
            "Episode 037/300 | Avg Reward: 38.25\n",
            "Episode 038/300 | Avg Reward: 37.29\n",
            "Episode 039/300 | Avg Reward: 38.03\n",
            "Episode 040/300 | Avg Reward: 37.09\n",
            "Episode 041/300 | Avg Reward: 37.26\n",
            "Episode 042/300 | Avg Reward: 37.02\n",
            "Episode 043/300 | Avg Reward: 37.71\n",
            "Episode 044/300 | Avg Reward: 37.49\n",
            "Episode 045/300 | Avg Reward: 37.71\n",
            "Episode 046/300 | Avg Reward: 37.72\n",
            "Episode 047/300 | Avg Reward: 37.71\n",
            "Episode 048/300 | Avg Reward: 37.37\n",
            "Episode 049/300 | Avg Reward: 36.73\n",
            "Episode 050/300 | Avg Reward: 36.86\n",
            "Episode 051/300 | Avg Reward: 36.73\n",
            "Episode 052/300 | Avg Reward: 37.96\n",
            "Episode 053/300 | Avg Reward: 36.97\n",
            "Episode 054/300 | Avg Reward: 37.02\n",
            "Episode 055/300 | Avg Reward: 36.74\n",
            "Episode 056/300 | Avg Reward: 38.40\n",
            "Episode 057/300 | Avg Reward: 38.20\n",
            "Episode 058/300 | Avg Reward: 37.77\n",
            "Episode 059/300 | Avg Reward: 38.20\n",
            "Episode 060/300 | Avg Reward: 37.30\n",
            "Episode 061/300 | Avg Reward: 37.32\n",
            "Episode 062/300 | Avg Reward: 37.89\n",
            "Episode 063/300 | Avg Reward: 37.71\n",
            "Episode 064/300 | Avg Reward: 38.35\n",
            "Episode 065/300 | Avg Reward: 37.04\n",
            "Episode 066/300 | Avg Reward: 37.28\n",
            "Episode 067/300 | Avg Reward: 36.44\n",
            "Episode 068/300 | Avg Reward: 38.31\n",
            "Episode 069/300 | Avg Reward: 37.80\n",
            "Episode 070/300 | Avg Reward: 39.25\n",
            "Episode 071/300 | Avg Reward: 38.34\n",
            "Episode 072/300 | Avg Reward: 37.38\n",
            "Episode 073/300 | Avg Reward: 38.29\n",
            "Episode 074/300 | Avg Reward: 37.71\n",
            "Episode 075/300 | Avg Reward: 36.31\n",
            "Episode 076/300 | Avg Reward: 37.73\n",
            "Episode 077/300 | Avg Reward: 37.87\n",
            "Episode 078/300 | Avg Reward: 37.93\n",
            "Episode 079/300 | Avg Reward: 37.24\n",
            "Episode 080/300 | Avg Reward: 37.52\n",
            "Episode 081/300 | Avg Reward: 37.75\n",
            "Episode 082/300 | Avg Reward: 37.85\n",
            "Episode 083/300 | Avg Reward: 37.34\n",
            "Episode 084/300 | Avg Reward: 36.93\n",
            "Episode 085/300 | Avg Reward: 37.62\n",
            "Episode 086/300 | Avg Reward: 38.00\n",
            "Episode 087/300 | Avg Reward: 37.20\n",
            "Episode 088/300 | Avg Reward: 38.02\n",
            "Episode 089/300 | Avg Reward: 36.93\n",
            "Episode 090/300 | Avg Reward: 37.61\n",
            "Episode 091/300 | Avg Reward: 37.51\n",
            "Episode 092/300 | Avg Reward: 37.43\n",
            "Episode 093/300 | Avg Reward: 38.00\n",
            "Episode 094/300 | Avg Reward: 37.59\n",
            "Episode 095/300 | Avg Reward: 37.50\n",
            "Episode 096/300 | Avg Reward: 36.52\n",
            "Episode 097/300 | Avg Reward: 36.99\n",
            "Episode 098/300 | Avg Reward: 37.34\n",
            "Episode 099/300 | Avg Reward: 37.62\n",
            "Episode 100/300 | Avg Reward: 37.77\n",
            "Episode 101/300 | Avg Reward: 37.90\n",
            "Episode 102/300 | Avg Reward: 37.47\n",
            "Episode 103/300 | Avg Reward: 37.29\n",
            "Episode 104/300 | Avg Reward: 38.30\n",
            "Episode 105/300 | Avg Reward: 38.04\n",
            "Episode 106/300 | Avg Reward: 37.70\n",
            "Episode 107/300 | Avg Reward: 38.18\n",
            "Episode 108/300 | Avg Reward: 38.00\n",
            "Episode 109/300 | Avg Reward: 37.86\n",
            "Episode 110/300 | Avg Reward: 37.44\n",
            "Episode 111/300 | Avg Reward: 37.00\n",
            "Episode 112/300 | Avg Reward: 36.34\n",
            "Episode 113/300 | Avg Reward: 37.80\n",
            "Episode 114/300 | Avg Reward: 36.91\n",
            "Episode 115/300 | Avg Reward: 37.88\n",
            "Episode 116/300 | Avg Reward: 38.59\n",
            "Episode 117/300 | Avg Reward: 36.67\n",
            "Episode 118/300 | Avg Reward: 38.56\n",
            "Episode 119/300 | Avg Reward: 36.63\n",
            "Episode 120/300 | Avg Reward: 38.44\n",
            "Episode 121/300 | Avg Reward: 36.43\n",
            "Episode 122/300 | Avg Reward: 37.56\n",
            "Episode 123/300 | Avg Reward: 37.51\n",
            "Episode 124/300 | Avg Reward: 36.84\n",
            "Episode 125/300 | Avg Reward: 36.80\n",
            "Episode 126/300 | Avg Reward: 37.55\n",
            "Episode 127/300 | Avg Reward: 38.05\n",
            "Episode 128/300 | Avg Reward: 37.74\n",
            "Episode 129/300 | Avg Reward: 37.40\n",
            "Episode 130/300 | Avg Reward: 37.13\n",
            "Episode 131/300 | Avg Reward: 37.55\n",
            "Episode 132/300 | Avg Reward: 37.62\n",
            "Episode 133/300 | Avg Reward: 37.41\n",
            "Episode 134/300 | Avg Reward: 37.78\n",
            "Episode 135/300 | Avg Reward: 36.47\n",
            "Episode 136/300 | Avg Reward: 38.65\n",
            "Episode 137/300 | Avg Reward: 37.81\n",
            "Episode 138/300 | Avg Reward: 38.16\n",
            "Episode 139/300 | Avg Reward: 37.25\n",
            "Episode 140/300 | Avg Reward: 37.85\n",
            "Episode 141/300 | Avg Reward: 37.51\n",
            "Episode 142/300 | Avg Reward: 36.74\n",
            "Episode 143/300 | Avg Reward: 37.07\n",
            "Episode 144/300 | Avg Reward: 38.23\n",
            "Episode 145/300 | Avg Reward: 38.18\n",
            "Episode 146/300 | Avg Reward: 37.40\n",
            "Episode 147/300 | Avg Reward: 38.10\n",
            "Episode 148/300 | Avg Reward: 37.62\n",
            "Episode 149/300 | Avg Reward: 37.04\n",
            "Episode 150/300 | Avg Reward: 36.70\n",
            "Episode 151/300 | Avg Reward: 38.22\n",
            "Episode 152/300 | Avg Reward: 38.08\n",
            "Episode 153/300 | Avg Reward: 38.00\n",
            "Episode 154/300 | Avg Reward: 37.53\n",
            "Episode 155/300 | Avg Reward: 38.48\n",
            "Episode 156/300 | Avg Reward: 37.96\n",
            "Episode 157/300 | Avg Reward: 36.64\n",
            "Episode 158/300 | Avg Reward: 37.23\n",
            "Episode 159/300 | Avg Reward: 37.66\n",
            "Episode 160/300 | Avg Reward: 37.41\n",
            "Episode 161/300 | Avg Reward: 37.40\n",
            "Episode 162/300 | Avg Reward: 37.68\n",
            "Episode 163/300 | Avg Reward: 37.22\n",
            "Episode 164/300 | Avg Reward: 37.23\n",
            "Episode 165/300 | Avg Reward: 37.78\n",
            "Episode 166/300 | Avg Reward: 38.09\n",
            "Episode 167/300 | Avg Reward: 37.27\n",
            "Episode 168/300 | Avg Reward: 36.77\n",
            "Episode 169/300 | Avg Reward: 38.65\n",
            "Episode 170/300 | Avg Reward: 37.71\n",
            "Episode 171/300 | Avg Reward: 36.98\n",
            "Episode 172/300 | Avg Reward: 37.28\n",
            "Episode 173/300 | Avg Reward: 38.14\n",
            "Episode 174/300 | Avg Reward: 37.16\n",
            "Episode 175/300 | Avg Reward: 37.91\n",
            "Episode 176/300 | Avg Reward: 37.81\n",
            "Episode 177/300 | Avg Reward: 37.98\n",
            "Episode 178/300 | Avg Reward: 37.08\n",
            "Episode 179/300 | Avg Reward: 37.49\n",
            "Episode 180/300 | Avg Reward: 37.65\n",
            "Episode 181/300 | Avg Reward: 37.80\n",
            "Episode 182/300 | Avg Reward: 37.36\n",
            "Episode 183/300 | Avg Reward: 37.18\n",
            "Episode 184/300 | Avg Reward: 37.71\n",
            "Episode 185/300 | Avg Reward: 37.97\n",
            "Episode 186/300 | Avg Reward: 37.73\n",
            "Episode 187/300 | Avg Reward: 37.74\n",
            "Episode 188/300 | Avg Reward: 37.19\n",
            "Episode 189/300 | Avg Reward: 37.02\n",
            "Episode 190/300 | Avg Reward: 37.49\n",
            "Episode 191/300 | Avg Reward: 37.89\n",
            "Episode 192/300 | Avg Reward: 37.75\n",
            "Episode 193/300 | Avg Reward: 37.87\n",
            "Episode 194/300 | Avg Reward: 37.97\n",
            "Episode 195/300 | Avg Reward: 36.31\n",
            "Episode 196/300 | Avg Reward: 37.98\n",
            "Episode 197/300 | Avg Reward: 38.88\n",
            "Episode 198/300 | Avg Reward: 38.30\n",
            "Episode 199/300 | Avg Reward: 36.87\n",
            "Episode 200/300 | Avg Reward: 37.46\n",
            "Episode 201/300 | Avg Reward: 37.68\n",
            "Episode 202/300 | Avg Reward: 38.02\n",
            "Episode 203/300 | Avg Reward: 36.93\n",
            "Episode 204/300 | Avg Reward: 36.07\n",
            "Episode 205/300 | Avg Reward: 37.13\n",
            "Episode 206/300 | Avg Reward: 37.71\n",
            "Episode 207/300 | Avg Reward: 38.02\n",
            "Episode 208/300 | Avg Reward: 38.46\n",
            "Episode 209/300 | Avg Reward: 37.73\n",
            "Episode 210/300 | Avg Reward: 36.91\n",
            "Episode 211/300 | Avg Reward: 37.84\n",
            "Episode 212/300 | Avg Reward: 36.82\n",
            "Episode 213/300 | Avg Reward: 37.46\n",
            "Episode 214/300 | Avg Reward: 37.66\n",
            "Episode 215/300 | Avg Reward: 37.86\n",
            "Episode 216/300 | Avg Reward: 37.89\n",
            "Episode 217/300 | Avg Reward: 37.56\n",
            "Episode 218/300 | Avg Reward: 36.95\n",
            "Episode 219/300 | Avg Reward: 37.53\n",
            "Episode 220/300 | Avg Reward: 36.98\n",
            "Episode 221/300 | Avg Reward: 36.15\n",
            "Episode 222/300 | Avg Reward: 37.05\n",
            "Episode 223/300 | Avg Reward: 36.31\n",
            "Episode 224/300 | Avg Reward: 37.82\n",
            "Episode 225/300 | Avg Reward: 37.18\n",
            "Episode 226/300 | Avg Reward: 36.93\n",
            "Episode 227/300 | Avg Reward: 37.49\n",
            "Episode 228/300 | Avg Reward: 37.90\n",
            "Episode 229/300 | Avg Reward: 38.07\n",
            "Episode 230/300 | Avg Reward: 37.79\n",
            "Episode 231/300 | Avg Reward: 36.81\n",
            "Episode 232/300 | Avg Reward: 37.52\n",
            "Episode 233/300 | Avg Reward: 35.97\n",
            "Episode 234/300 | Avg Reward: 38.21\n",
            "Episode 235/300 | Avg Reward: 36.72\n",
            "Episode 236/300 | Avg Reward: 38.03\n",
            "Episode 237/300 | Avg Reward: 37.38\n",
            "Episode 238/300 | Avg Reward: 37.48\n",
            "Episode 239/300 | Avg Reward: 38.13\n",
            "Episode 240/300 | Avg Reward: 37.09\n",
            "Episode 241/300 | Avg Reward: 36.53\n",
            "Episode 242/300 | Avg Reward: 36.98\n",
            "Episode 243/300 | Avg Reward: 37.17\n",
            "Episode 244/300 | Avg Reward: 37.87\n",
            "Episode 245/300 | Avg Reward: 38.13\n",
            "Episode 246/300 | Avg Reward: 38.68\n",
            "Episode 247/300 | Avg Reward: 37.50\n",
            "Episode 248/300 | Avg Reward: 37.46\n",
            "Episode 249/300 | Avg Reward: 37.95\n",
            "Episode 250/300 | Avg Reward: 36.81\n",
            "Episode 251/300 | Avg Reward: 37.56\n",
            "Episode 252/300 | Avg Reward: 38.37\n",
            "Episode 253/300 | Avg Reward: 37.61\n",
            "Episode 254/300 | Avg Reward: 38.20\n",
            "Episode 255/300 | Avg Reward: 37.52\n",
            "Episode 256/300 | Avg Reward: 37.16\n",
            "Episode 257/300 | Avg Reward: 38.23\n",
            "Episode 258/300 | Avg Reward: 38.62\n",
            "Episode 259/300 | Avg Reward: 37.44\n",
            "Episode 260/300 | Avg Reward: 37.23\n",
            "Episode 261/300 | Avg Reward: 37.10\n",
            "Episode 262/300 | Avg Reward: 37.32\n",
            "Episode 263/300 | Avg Reward: 37.13\n",
            "Episode 264/300 | Avg Reward: 38.02\n",
            "Episode 265/300 | Avg Reward: 38.12\n",
            "Episode 266/300 | Avg Reward: 37.44\n",
            "Episode 267/300 | Avg Reward: 37.80\n",
            "Episode 268/300 | Avg Reward: 38.37\n",
            "Episode 269/300 | Avg Reward: 36.92\n",
            "Episode 270/300 | Avg Reward: 37.28\n",
            "Episode 271/300 | Avg Reward: 37.51\n",
            "Episode 272/300 | Avg Reward: 37.72\n",
            "Episode 273/300 | Avg Reward: 37.39\n",
            "Episode 274/300 | Avg Reward: 37.70\n",
            "Episode 275/300 | Avg Reward: 37.18\n",
            "Episode 276/300 | Avg Reward: 38.71\n",
            "Episode 277/300 | Avg Reward: 38.32\n",
            "Episode 278/300 | Avg Reward: 37.56\n",
            "Episode 279/300 | Avg Reward: 37.83\n",
            "Episode 280/300 | Avg Reward: 38.20\n",
            "Episode 281/300 | Avg Reward: 37.70\n",
            "Episode 282/300 | Avg Reward: 37.75\n",
            "Episode 283/300 | Avg Reward: 37.18\n",
            "Episode 284/300 | Avg Reward: 38.07\n",
            "Episode 285/300 | Avg Reward: 37.88\n",
            "Episode 286/300 | Avg Reward: 38.32\n",
            "Episode 287/300 | Avg Reward: 37.19\n",
            "Episode 288/300 | Avg Reward: 37.95\n",
            "Episode 289/300 | Avg Reward: 38.04\n",
            "Episode 290/300 | Avg Reward: 38.08\n",
            "Episode 291/300 | Avg Reward: 38.01\n",
            "Episode 292/300 | Avg Reward: 36.09\n",
            "Episode 293/300 | Avg Reward: 37.00\n",
            "Episode 294/300 | Avg Reward: 38.78\n",
            "Episode 295/300 | Avg Reward: 37.49\n",
            "Episode 296/300 | Avg Reward: 36.90\n",
            "Episode 297/300 | Avg Reward: 37.11\n",
            "Episode 298/300 | Avg Reward: 38.73\n",
            "Episode 299/300 | Avg Reward: 38.64\n",
            "Episode 300/300 | Avg Reward: 38.37\n",
            "Training completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION WITH TEST DATA"
      ],
      "metadata": {
        "id": "asBqhL1tPojE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import deque\n",
        "import random\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "PROMOTION_IDS = ['PR100', 'PR101', 'PR102', 'PR103', 'PR104',\n",
        "                'PR105', 'PR106', 'PR107', 'PR108', 'PR109']\n",
        "\n",
        "class AdvancedRewardSystem:\n",
        "    def __init__(self, config):\n",
        "        self.weights = config['weights']\n",
        "        self.segment_multipliers = config['segment_multipliers']\n",
        "        self.promo_costs = {\n",
        "            'PR100': 0.15, 'PR101': 0.20, 'PR102': 0.18,\n",
        "            'PR103': 0.25, 'PR104': 0.22, 'PR105': 0.19,\n",
        "            'PR106': 0.17, 'PR107': 0.21, 'PR108': 0.23,\n",
        "            'PR109': 0.16\n",
        "        }\n",
        "        self.eligibility_map = {\n",
        "            'Low': ['PR100', 'PR101', 'PR102'],\n",
        "            'Medium': ['PR103', 'PR104', 'PR105'],\n",
        "            'High': ['PR106', 'PR107', 'PR108', 'PR109']\n",
        "        }\n",
        "\n",
        "    def get_valid_actions(self, promotion_history):\n",
        "        eligible = self.eligibility_map.get(promotion_history, [])\n",
        "        return [PROMOTION_IDS.index(p) for p in eligible if p in PROMOTION_IDS]\n",
        "\n",
        "    def calculate_reward(self, transaction, action):\n",
        "        promo_id = PROMOTION_IDS[action]\n",
        "        cost = self.promo_costs[promo_id]\n",
        "\n",
        "        response = transaction['response']\n",
        "        quantity = transaction['quantity']\n",
        "        price = transaction['price']\n",
        "        margin = transaction['margin'] - cost\n",
        "        loyalty = transaction['loyalty_score']\n",
        "        churn = transaction['churn_score']\n",
        "        segment = transaction.get('segment', 'Standard')\n",
        "\n",
        "        sales_reward = math.log1p(quantity * price) * response * 2.0\n",
        "        margin_reward = margin * quantity * 0.5\n",
        "        retention_reward = (1 / (1 + math.exp(-loyalty))) - math.exp(churn)\n",
        "        penalty = (1 - response) * price * 0.3\n",
        "\n",
        "        segment_mult = self.segment_multipliers.get(segment, 1.0)\n",
        "        return ((self.weights['sales'] * sales_reward +\n",
        "                self.weights['margin'] * margin_reward +\n",
        "                self.weights['retention'] * retention_reward -\n",
        "                penalty) * segment_mult)\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, state_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = len(PROMOTION_IDS)\n",
        "        self.actor = self._build_actor()\n",
        "\n",
        "    def _build_actor(self):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(self.state_size, 512),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(256, self.action_size)\n",
        "        )\n",
        "\n",
        "    def act(self, state, valid_actions, deterministic=False):\n",
        "        state = torch.FloatTensor(state)\n",
        "        with torch.no_grad():\n",
        "            logits = self.actor(state)\n",
        "            valid_actions_tensor = torch.tensor(valid_actions, dtype=torch.long)\n",
        "            masked_logits = torch.full_like(logits, -1e8)\n",
        "            masked_logits[valid_actions_tensor] = logits[valid_actions_tensor]\n",
        "\n",
        "            if deterministic:\n",
        "                action = masked_logits.argmax(dim=-1)\n",
        "            else:\n",
        "                dist = Categorical(logits=masked_logits)\n",
        "                action = dist.sample()\n",
        "\n",
        "        return action.item()\n",
        "\n",
        "def preprocess_data(df, train_stats=None):\n",
        "    df = df.drop(['user_id', 'product_id', 'promotion_used'], axis=1, errors='ignore')\n",
        "    categorical_cols = ['category', 'gender', 'segment', 'promotion_history']\n",
        "    df = pd.get_dummies(df, columns=categorical_cols)\n",
        "    numeric_cols = ['quantity', 'price', 'margin', 'churn_score',\n",
        "                   'loyalty_score', 'avg_order_value', 'age', 'income']\n",
        "\n",
        "    if train_stats:\n",
        "        means = train_stats['means']\n",
        "        stds = train_stats['stds']\n",
        "    else:\n",
        "        means = df[numeric_cols].mean()\n",
        "        stds = df[numeric_cols].std()\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = (df[col] - means[col]) / stds[col]\n",
        "\n",
        "    return df.fillna(0), (means, stds) if train_stats is None else None\n",
        "\n",
        "def evaluate_model(test_path, model_path, reward_config, train_stats):\n",
        "    df_test = pd.read_csv(test_path)\n",
        "    df_test = df_test[df_test['promotion_used'].isin(PROMOTION_IDS)].copy()\n",
        "    processed_test, _ = preprocess_data(df_test, train_stats)\n",
        "    state_columns = processed_test.columns.tolist()\n",
        "    states_test = processed_test.values.astype(np.float32)\n",
        "    states_test = np.nan_to_num(states_test)\n",
        "    reward_system = AdvancedRewardSystem(reward_config)\n",
        "\n",
        "    agent = PPOAgent(states_test.shape[1])\n",
        "    agent.actor.load_state_dict(torch.load(model_path))\n",
        "    agent.actor.eval()\n",
        "\n",
        "    total_reward = 0\n",
        "    correct_predictions = 0\n",
        "    action_counts = {p: 0 for p in PROMOTION_IDS}\n",
        "\n",
        "    for idx in range(len(df_test)):\n",
        "        row = df_test.iloc[idx]\n",
        "        state = states_test[idx]\n",
        "        valid_actions = reward_system.get_valid_actions(row['promotion_history'])\n",
        "        if not valid_actions:\n",
        "            continue\n",
        "        action = agent.act(state, valid_actions, deterministic=True)\n",
        "        promo_used = PROMOTION_IDS[action]\n",
        "        action_counts[promo_used] += 1\n",
        "\n",
        "        reward = reward_system.calculate_reward(row.to_dict(), action)\n",
        "        total_reward += reward\n",
        "\n",
        "        if promo_used == row['promotion_used']:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    avg_reward = total_reward / len(df_test) if len(df_test) > 0 else 0\n",
        "    accuracy = (correct_predictions / len(df_test)) * 100 if len(df_test) > 0 else 0\n",
        "\n",
        "    print(f\"\\n{' Evaluation Results ':=^40}\")\n",
        "    print(f\"Test Samples: {len(df_test)}\")\n",
        "    print(f\"Total Reward: {total_reward:.2f}\")\n",
        "    print(f\"Avg Reward/Sample: {avg_reward:.2f}\")\n",
        "    print(f\"Accuracy vs Historical: {accuracy:.2f}%\")\n",
        "\n",
        "    print(\"\\nAction Distribution:\")\n",
        "    for promo, count in action_counts.items():\n",
        "        print(f\"{promo}: {count} ({count/len(df_test)*100:.1f}%)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_df = pd.read_csv('train_data.csv')\n",
        "    _, train_stats = preprocess_data(train_df)\n",
        "\n",
        "    reward_config = {\n",
        "        'weights': {'sales': 0.4, 'margin': 0.35, 'retention': 0.25},\n",
        "        'segment_multipliers': {'Premium': 1.6, 'Standard': 1.1, 'Budget': 0.7}\n",
        "    }\n",
        "\n",
        "    evaluate_model(\n",
        "        test_path='test_data.csv',\n",
        "        model_path='final_promotion_model.pth',\n",
        "        reward_config=reward_config,\n",
        "        train_stats={'means': train_stats[0], 'stds': train_stats[1]}\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spGE7ZyvPuYN",
        "outputId": "aefb28b6-1b92-446c-afeb-13078967644e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Evaluation Results ==========\n",
            "Test Samples: 43\n",
            "Total Reward: 1377.31\n",
            "Avg Reward/Sample: 32.03\n",
            "Accuracy vs Historical: 13.95%\n",
            "\n",
            "Action Distribution:\n",
            "PR100: 6 (14.0%)\n",
            "PR101: 5 (11.6%)\n",
            "PR102: 5 (11.6%)\n",
            "PR103: 0 (0.0%)\n",
            "PR104: 4 (9.3%)\n",
            "PR105: 5 (11.6%)\n",
            "PR106: 10 (23.3%)\n",
            "PR107: 1 (2.3%)\n",
            "PR108: 0 (0.0%)\n",
            "PR109: 7 (16.3%)\n"
          ]
        }
      ]
    }
  ]
}